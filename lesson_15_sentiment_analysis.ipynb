{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "# Text classification: sentiment analysis \n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Popular tasks of text classification\n",
    "\n",
    "</font>\n",
    "\n",
    "- **Spam detection**: Having message decide is is spammy or not \n",
    "- **Topic identification**: Having article choose one of known classes like \"Sport\", \"Technology\", \"Finances\"\n",
    "- **Sentiment analysis**: Is the moview positive or negative \n",
    "- **Spelling correction**: what is more suitable \"weather\" or \"whether\"  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Features from Text\n",
    "\n",
    "</font>\n",
    "\n",
    "1. The most common words\n",
    "2. *Stop* words\n",
    "3. Normalization: lower case / stemming / lemmatizing\n",
    "4. Capitalization as feature \n",
    "5. POS e.g. \"the weather\" vs whether  \n",
    "6. grouping\n",
    "    - buy, purchase\n",
    "    - Mr, Ms, Dr\n",
    "    - Numbers\n",
    "    - Dates\n",
    "7. Bigrams, n-grams e.g. \"White House\"\n",
    "8. Sub-sequences e.g. \"ing\", \"ion\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Naive Bayes Classifiers\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Text classyfication of search query \n",
    "\n",
    "</font>\n",
    "\n",
    "- **python**  as snake -> Zoology\n",
    "- **python**  as programming language -> Computer Science\n",
    "- **python**  as \"monty python\" -> Entertainment\n",
    "\n",
    "Probabilistic model:\n",
    "\n",
    "#### Bayes Rule\n",
    "\n",
    "\\begin{equation*}\n",
    "P(y|X) = \\frac{P(X| y) \\cdot P(y)}{P(X)} \n",
    "\\quad\\quad\\quad\n",
    "Posterior = \\frac{ Likelihood \\cdot Prior}{Evidence} \n",
    "\\quad\\quad\\quad\n",
    "P(class| python) = \\frac{P(python| class) \\cdot P(class)}{P(python)} \n",
    "\\end{equation*}\n",
    "\n",
    "Considering the $P(python)$ is common for all classes we may compare just nominators: \n",
    "\n",
    "\\begin{equation*}\n",
    "P(python| Zoology) \\cdot P(Zoology) \n",
    "\\quad\\quad\\quad \n",
    "P(python|CS) \\cdot P(CS) \n",
    "\\quad\\quad\\quad\n",
    "P(python|Entertainment) \\cdot P(Entertainment) \n",
    "\\end{equation*}\n",
    "\n",
    "In general: \n",
    "\\begin{equation*}\n",
    "\\hat{y} =  \\underset{y}{argmax} \\quad P(y|X) =  \\underset{argmax}{y} P(X|y) \\cdot P(y)\n",
    "\\end{equation*}\n",
    "\n",
    "Most probably predicted class is <font color = blue>Zoology</font>\n",
    "\n",
    "#### Naive Bayes Classifiers\n",
    "\\begin{equation*}\n",
    "\\hat{y} =  \\underset{y}{argmax} \\quad P(y) \\prod_{ i=1 }^{ n }{ P(x_{ i }|\\,y) } \n",
    "\\end{equation*}\n",
    "\n",
    "If search query = **\"python download\"** \n",
    "\\begin{equation*}\n",
    "\\hat{y} =  \\underset{y}{argmax} \\quad\n",
    "P(y)\\cdot P(python|\\,y) \\cdot P(download|\\,y)\n",
    "\\end{equation*}\n",
    "\n",
    "Now, the most probably predicted class is <font color = blue>Computer Science</font> since  $P(download|\\,Zoology)$ is far less than $P(download|\\,CS)$\n",
    "\n",
    "Note: if one of word is not presented in text then its statistical propability = 0 and as the result\n",
    "the whole likelihood = 0 regardless of other words. Thus it is worth using laplace smooting  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Laplace smooting \n",
    " \n",
    "</font>\n",
    "\n",
    "\n",
    "$\n",
    "A : 1 \\quad\n",
    "B : 3\\quad\n",
    "C : 0\\quad\n",
    "D : 6\\quad\n",
    "$\n",
    "\n",
    "$N= 10\\quad K =4$ \n",
    "<br>N - number of samples, K - number of classes\n",
    "\n",
    "\\begin{equation*}\n",
    "P(A) = 0.1\\quad\\quad\\quad\\quad\n",
    "P(B) = 0.3\\quad\\quad\\quad\\quad\n",
    "P(C) = 0.0\\quad\\quad\\quad\\quad\n",
    "P(D) = 0.6\\\\\n",
    "\\end{equation*}\n",
    "\n",
    "<font color = blue >\n",
    "\n",
    "\\begin{equation*}\n",
    "P^{\\,L}(x_{i}) =  \\frac{P(x_{i})+1}{N+K}\n",
    "\\end{equation*}\n",
    "\n",
    "</font>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "P^{\\,L}(A) =  \\frac{1+1}{10+4} = 0.14 \\quad P^{\\,L}(B) =  \\frac{3+1}{10+4} = 0.29\n",
    "\\quad P^{\\,L}(C) =  \\frac{0+1}{10+4} = 0.07 \\quad P^{\\,L}(D) =  \\frac{6+1}{10+4} = 0.5\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Sentiment Analysis\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Using NLTK\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from nltk.corpus import movie_reviews \n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Load data\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_movie_reviews_text= movie_reviews.raw()\n",
    "all_movie_reviews_text[:600]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Tools to review data \n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats =  movie_reviews.categories()\n",
    "cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = cats[0]\n",
    "ids= movie_reviews.fileids(cat)\n",
    "ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_review = ids[0]\n",
    "print(movie_reviews.raw(id_review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Tokenize\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text): # removes punctualtion\n",
    "    tokenizer = RegexpTokenizer(r'\\w+') # just for demo \n",
    "    return tokenizer.tokenize(text.lower())\n",
    "\n",
    "all_words = preprocess(all_movie_reviews_text)\n",
    "print (len(all_words))\n",
    "print(all_words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Build vocabulary\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words=nltk.FreqDist(all_words)\n",
    "print ('len of vocabulary: {:,}'.format (len(all_words)))\n",
    "# Use most common words\n",
    "most_common_words = list(zip(*all_words.most_common()))[0] # [0] means names whereas [1] are frequencies \n",
    "# most_common(5000) - it may retutn limited number but in this sample the features will be filtered later after removing stop words \n",
    "print (most_common_words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Get rid of stop words \n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(words):\n",
    "    stop_words = set(stopwords.words('english'))  \n",
    "    return [w for w in words if w not in stop_words]\n",
    "\n",
    "most_common_words_filtered = remove_stop_words(most_common_words)\n",
    "word_features = most_common_words_filtered [:3000]\n",
    "print (word_features[:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Extract documents and labels\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this dopes not use tokenizing to documents but words of document retrieved by file_id instead.\n",
    "documents = [(list(movie_reviews.words(file_id)), category) # using the words() method of movie_reviews object\n",
    "             for category in movie_reviews.categories() # select category - there are two: ['neg', 'pos']\n",
    "             for file_id in movie_reviews.fileids(category)]# select all file_ids for specified category\n",
    "len (documents)\n",
    "# This returns list of tuples (list_of_tokens_of document, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (documents [0]) # (['plot', ':', 'two', 'teen', ... 'echoes', '(', '8', '/', '10', ')'], 'neg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Shuffle documents \n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle first \n",
    "random.shuffle(documents) # it is inplace method\n",
    "documents= documents[:500] # reduce the data set for speed up the demo \n",
    "len (documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Vectorize documents \n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(review_tokens):\n",
    "    return {w: w in set(review_tokens) for w in word_features} # feature representation on document\n",
    "\n",
    "data_set= [(find_features(review_tokens), category) for (review_tokens, category) in documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Split to training and test set\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_on = int(len(data_set)*.8)\n",
    "X_y_train= data_set[:split_on]\n",
    "X_y_test = data_set[split_on:]\n",
    "print (len(X_y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Train model\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf= nltk.NaiveBayesClassifier.train(X_y_train) # Note: the difference grammar comparing with sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Evaluate model\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.classify.accuracy(clf, X_y_test)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Review most informative features\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Incorporate with sklearn\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier # this is wrapper to incorporate with sklearn using nltk style.\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Convert to nltk classifiers \n",
    "MNNB_classifier= SklearnClassifier(MultinomialNB()) # Note : use ()\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_classifier = SklearnClassifier(LogisticRegression()) \n",
    "\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC # NuSVC - Similar to SVC but uses a parameter to control the number of support vectors.\n",
    "svc_clf = SklearnClassifier(SVC())  \n",
    "lin_svc_clf= SklearnClassifier(LinearSVC())  \n",
    "nu_svc_clf = SklearnClassifier(NuSVC())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# native nltk classifier\n",
    "clf= nltk.NaiveBayesClassifier.train(X_y_train) \n",
    "\n",
    "print('Accuracy nltk.NaiveBayesClassifier={}%'.format(nltk.classify.accuracy(clf,X_y_test) * 100))\n",
    "# clf.show_most_informative_features(15)\n",
    "\n",
    "MNNB_classifier.train(X_y_train)\n",
    "print('Accuracy MNNB_classifier ={}%'.format(nltk.classify.accuracy(MNNB_classifier, X_y_test) * 100)) # 79.0%\n",
    "\n",
    "lr_classifier.train(X_y_train)\n",
    "print('Accuracy lr_classifier ={}%'.format(nltk.classify.accuracy(lr_classifier, X_y_test) * 100)) # 82.0%\n",
    "\n",
    "svc_clf.train(X_y_train)\n",
    "print('Accuracy svc_clf={}%'.format(nltk.classify.accuracy(svc_clf, X_y_test) * 100)) # 52.0% - default is rbf kernel\n",
    "\n",
    "lin_svc_clf.train(X_y_train)\n",
    "print('Accuracy lin_svc_clf={}%'.format(nltk.classify.accuracy(lin_svc_clf, X_y_test) * 100)) # 82.0%\n",
    "\n",
    "nu_svc_clf.train(X_y_train)\n",
    "print('Accuracy nu_svc_clf={}%'.format(nltk.classify.accuracy(nu_svc_clf, X_y_test) * 100)) #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Combining algos with a vote\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import ClassifierI\n",
    "class Vote_Classifier(ClassifierI): # inherit\n",
    "    def __init__(self, *classifiers): # expecting list of classifiers\n",
    "        self.classifiers = classifiers\n",
    "\n",
    "    def classify(self, sample): \n",
    "        return mode([clf.classify(sample) for clf in self.classifiers]) \n",
    "\n",
    "    def calc_confidence(self, sample):\n",
    "        votes= [clf.classify(sample) for clf in self.classifiers] #\n",
    "        return votes.count(mode(votes))/len(votes) # fraction of how many votes match to mode to total votes number\n",
    "\n",
    "def mode(array): # returns first mode in case of multi modes\n",
    "    return max(set(array), key=array.count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_clf= Vote_Classifier(clf, lr_classifier, svc_clf, lin_svc_clf, nu_svc_clf)\n",
    "print('Accuracy vote_clf={:.2%}'.format(nltk.classify.accuracy(vote_clf, X_y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Classify new sample\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://timesofindia.indiatimes.com/entertainment/english/movie-reviews/cold-pursuit/movie-review/67892834.cms\n",
    "new_review = '''This unusual satire on gangsters and revenge stories, starts off with a quote from Oscar Wilde and some delightful background music that sets the tone for rest of the film. The first few minutes play out like a predictable thriller, featuring a wronged father and his pursuit for vigilante justice. But, what follows is a series of stylised killing sequences, that almost seem like parodies of action set pieces that you’ve seen Liam Neeson pulling off with deadpan ease in the past. Yet, director Hans Petter Noland, who also made the Norwegian film In Order Of Disappearance that inspired Cold Pursuit, and writer Frank Baldwin create a refreshing narrative full of memorable moments. The movie seems bizarrely funny and the snow-heavy setting creates the right atmosphere for the dry and cold-cut humour.\n",
    "The story begins with tragedy and the first few minutes seem dead serious, right up to the point where Coxman confronts his first victim, the gangster named Speedo. But, as the revenge-seeking father moves up the ranks of the mafia chain, the characters become quirky and the situations get thoroughly entertaining. The introduction of characters like Viking (Tom Bateman), the main antagonist and his team of crazy henchmen like Mustang, Dexter and more, alleviates the narrative. There’s also a track of warring mafia gangs as Viking wages a war against the native Indians led by White Bull (Tom Jackson). Cold Pursuit may not be too creative with the kill sequences, but it does get interesting with the wry sense of humour.\n",
    "Neeson does what he does best. He keeps a straight face and plays the game of intimidation with ease. He’s just a regular guy who’s way out of his league, killing gangsters. But, his outrageous mission is what makes the story interesting. Watch out for a superb cameo by William Forsythe, too, who plays a brief but key role in Coxman’s revenge saga.\n",
    "The way Cold Pursuit manages to blend sardonic humour with cold-blooded killings makes it reminiscent of movies like The Coen Brothers’ Fargo and Guy Ritchie’s Snatch. This one’s a refreshingly cool black-comedy that does wonders for the genre.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new  = find_features(preprocess(new_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nClassification: {}\\nConfidence: {:.2%}'.format(\n",
    "    vote_clf.classify(x_new),vote_clf.calc_confidence(x_new)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Using sklearn\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Load data \n",
    "\n",
    "data set ['amazon-reviews-unlocked-mobile-phones'](https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones)\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd= os.getcwd() # current working directory\n",
    "path = os.path.join(cwd,'data')\n",
    "fn=  os.path.join(path , 'Amazon_Unlocked_Mobile.csv') # https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones\n",
    "df = pd.read_csv(fn) # \n",
    "print('len=  {:,}\\ncolumns= {}'.format(len(df), list(df)))\n",
    "\n",
    "# df = df.sample(frac=0.1, random_state=10) # reduce the amount of reviews due to speedup the training considering this is demo\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Get rid of records with missed data \n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True) \n",
    "print('len=  {:,}'.format(len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Label positive and negative \n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Rating'] != 3] # Remove any 'neutral' ratings equal to 3  as uninformative\n",
    "df['Rating_binary'] = np.where(df['Rating'] > 3, 1, 0) # returns 1 for 4,5 and 0 for 1,2\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Rating_binary'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Split to train and test sets\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Reviews'],df['Rating_binary'],random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Review training sample\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Extract Features \n",
    "\n",
    "</font>\n",
    "The bag-of-words approach is simple way to represent text for use in machine learning, which ignores structure and only counts how often each word occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Count vectorizer\n",
    "\n",
    "</font>\n",
    "By default, selects tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "vect = CountVectorizer().fit(X_train) # Fit the CountVectorizer to the training data\n",
    "print('features samples:\\n{}'.format(vect.get_feature_names()[::2000]))\n",
    "print ('\\nlen of features {:,}'.format(len(vect.get_feature_names()))) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Transfrom the X_train to feature representation\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectorized = vect.transform(X_train) # indeces of existing words from vocabulary and their count in current text\n",
    "X_train_vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Review vectorized training sample\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review first sample \n",
    "df = pd.DataFrame(X_train_vectorized[0].toarray(), index= ['value']).T\n",
    "print (list(df[df['value']>0].index))\n",
    "[vect.get_feature_names()[index] for index in df[df['value']>0].index.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Train model\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression().fit(X_train_vectorized, y_train) # Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Evaluate model\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(vect.transform(X_test)) # Predict the transformed test documents\n",
    "print('f1: ', f1_score(y_test, predictions)) \n",
    "scores = clf.decision_function(vect.transform(X_test)) \n",
    "print('AUC: ', roc_auc_score(y_test, scores)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Review relevant features \n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = np.array(vect.get_feature_names())\n",
    "sorted_coef_index = clf.coef_[0].argsort() # ascending  [0] is just squeeze from shape (1,n)\n",
    "print('Smallest coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))\n",
    "# model.coef_[0][sorted_coef_index[0]] the smallest "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Term frequency–inverse document frequency (TFIDF)\n",
    "\n",
    "\n",
    "</font>\n",
    "\n",
    "TFIDF is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. Its value increases proportionally to the number of times a word appears in the document and is decreases by the number of documents in the corpus that contain the word\n",
    "<div style=\"float:left;\">\n",
    "<br>\n",
    "    \n",
    "**Term frequency** $(tf(t,d))$ is measure of how frequent term t is in document d \n",
    "$$ tf(t,d) = \\frac{k}{n},$$ <br>$d$ - document,  $k$ - number of times word occurs in document $d$, $n$ - total number of words in document $d$.\n",
    "<br>\n",
    "Note: Various approaches can be used for term frequency e.g. *augmented frequency*, to prevent a bias towards longer documents (raw frequency divided by the raw frequency of the most occurring term in the document):\n",
    "\n",
    "$$ tf^{\\,A}(t,d) = 0.5+ 0.5\\cdot \\frac{tf(t,d)}{\\underset{t' \\in d}{max}(tf(t',d))} $$\n",
    "\n",
    "**Inverse document frequency** $(idf(t,D))$ is a measure of how much information the word provides.\n",
    "$$ idf(t,D) = log \\frac{N}{K},$$ <br>$D$ - all documents, $K$ - number of documents in $D$ that contain the word , $N$ - total number of documents in $D$. <br>\n",
    "</div>\n",
    "\n",
    "Note: Various approaches can be used for inverse document frequency \n",
    "\n",
    "<div style=\"float:left;\">\n",
    "<table width=\"500\">\n",
    "    <tr>\n",
    "        <th style=\"text-align:center\" bgcolor= white>Document1</th>\n",
    "        <th style=\"text-align:center\"  bgcolor= white >Document2</th></tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <table>\n",
    "                <tr>\n",
    "                    <th  bgcolor=gainsboro>Term</th>\n",
    "                    <th  bgcolor=gainsboro>Term Count</th></tr>\n",
    "                <tr><td>this</td><td>1</td></tr>\n",
    "                <tr><td>is</td><td>1</td></tr>\n",
    "                <tr><td>a</td><td>2</td></tr>\n",
    "                <tr><td>sample</td><td>1</td></tr>\n",
    "            </table>\n",
    "        </td>\n",
    "        <td>\n",
    "            <table>\n",
    "                <tr>\n",
    "                    <th bgcolor=gainsboro>Term</th>\n",
    "                    <th  bgcolor=gainsboro>Term Count</th></tr>\n",
    "                <tr><td>this</td><td>1</td></tr>\n",
    "                <tr><td>is</td><td>1</td></tr>\n",
    "                <tr><td>another</td><td>2</td></tr>\n",
    "                <tr><td>example</td><td>3</td></tr>\n",
    "            </table>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<div/>\n",
    "\n",
    "<div style=\"float:left;\">\n",
    "<br>\n",
    "\n",
    "For <strong>\"this\"</strong>:\n",
    "$$ tf (\"this\", d_{1}) =  \\frac{1}{5} = 0.2, \\quad  tf (\"this\", d_{2}) =  \\frac{1}{7} \\approx 0.14, \\quad idf (\"this\", D) =  log \\frac{2}{2} =0; $$\n",
    "\n",
    "$$ tfidf(\"this\", d_{1}, D)  = 0.2 \\cdot 0 = 0, \\quad    tfidf(\"this\", d_{2}, D)  = 0.14 \\cdot 0 = 0 $$\n",
    "\n",
    "For <strong>\"example\"</strong>:\n",
    "$$ tf (\"example\", d_{1}) =  \\frac{0}{5} = 0 , \\quad  tf (\"example\", d_{2}) =  \\frac{3}{7} \\approx 0.43 , \\quad idf (\"example\", D) =  log \\frac{2}{1} \\approx 0.3; $$\n",
    "\n",
    "$$ tfidf(\"example\", d_{1}, D)  = 0 \\cdot 0.3 = 0, \\quad    tfidf(\"example\", d_{2}, D)  = 0.43 \\cdot 0.3 = 0.129 $$\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Sklearn tfidf\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Compute sklearn tfidf for sample with 2 documents \n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(['this is a sample a', 'this is another example another example example'])\n",
    "tfidf_vectorizer= TfidfVectorizer().fit(X)\n",
    "X_vectorized= tfidf_vectorizer.transform(X)\n",
    "print (tfidf_vectorizer.vocabulary_)\n",
    "X_vectorized.toarray()\n",
    "# conclusion: sklearn uses different variant of computation tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Use sklearn tfidf for Amazon_Unlocked_Mobile documents \n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer= TfidfVectorizer(min_df=5)#.fit(X_train) \n",
    "    # min_df - minimum document count to include the term, default is 1 \n",
    "    # you may also set max_features (Int or None) to return just limited number of top tfidf features \n",
    "X_train_vectorized = tfidf_vectorizer.fit_transform(X_train)\n",
    "print ('len of features= {:,}'.format(len(tfidf_vectorizer.get_feature_names()))) \n",
    "    # Note: min_df=5 caused 17,951  comparing to 53,216 acquired by count vectorizer\n",
    "    # Note: min_df=5 is also available in count vectorizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_vectorized.shape # (231207, 17951) = (n_documents, n_features)\n",
    "sorted_tfidf_index = X_train_vectorized.max(axis=0).toarray()[0].argsort() \n",
    "    # max(axis=0) means max through all docs - will get the max of each word within all docs\n",
    "    # [0] - just squeezing     \n",
    "print (np.sort(X_train_vectorized.max(axis=0).toarray()[0]))\n",
    "sorted_tfidf_index # indices of the most tfidf terms \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = np.array(tfidf_vectorizer.get_feature_names())\n",
    "print ('feature_names ',feature_names)\n",
    "print('Smallest tfidf:\\n{}\\n'.format(feature_names[sorted_tfidf_index[:10]]))\n",
    "print('Largest tfidf: \\n{}'.format(feature_names[sorted_tfidf_index[:-11:-1]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Train model on features  extracted by tfidf vectorizer\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression().fit(X_train_vectorized, y_train) # Train the model\n",
    "predictions = clf.predict(tfidf_vectorizer.transform(X_test)) \n",
    "print('f1: ', f1_score(y_test, predictions)) \n",
    "scores = clf.decision_function(tfidf_vectorizer.transform(X_test)) \n",
    "print('AUC: ', roc_auc_score(y_test, scores)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusion: Perfromance is not worse but features used is 3 times less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_coef_index = clf.coef_[0].argsort()\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### n-grams\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the problem is the follwoing reviews are treated the same by current model\n",
    "targets= [\n",
    "    \"not an issue, phone is working\", \n",
    "    \"an issue, phone is not working\"\n",
    "]\n",
    "print(clf.predict(tfidf_vectorizer.transform(targets)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(min_df=5, max_features=50000, ngram_range=(1,2)).fit(X_train) # Note: both limits are included\n",
    "X_train_vectorized = count_vectorizer.transform(X_train)\n",
    "print('len of features using n-grams vectorizer={:,}'.format(len(count_vectorizer.get_feature_names()))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf= LogisticRegression().fit(X_train_vectorized, y_train)\n",
    "predictions = clf.predict(count_vectorizer.transform(X_test)) \n",
    "print('f1: ', f1_score(y_test, predictions)) \n",
    "scores = clf.decision_function(count_vectorizer.transform(X_test)) \n",
    "print('AUC: ', roc_auc_score(y_test, scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = np.array(count_vectorizer.get_feature_names())\n",
    "sorted_coef_index = clf.coef_[0].argsort()\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (targets)\n",
    "print(clf.predict(count_vectorizer.transform(targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Home Task \n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Load data \n",
    "\n",
    "</font>\n",
    "\n",
    "[Sentiment Analysis Dataset](https://www.kaggle.com/sonaam1234/sentimentdata)\n",
    "\n",
    "alternative source: \n",
    "<br>\n",
    "[rt-polaritydata](https://github.com/dennybritz/cnn-text-classification-tf/tree/master/data/rt-polaritydata)\n",
    "\n",
    "alternative source: \n",
    "<br>\n",
    "[Movie Review Data](http://www.cs.cornell.edu/people/pabo/movie-review-data)\n",
    "\n",
    "Each line in these two files corresponds to a single snippet (usually containing roughly one single sentence); all snippets are down-cased.  \n",
    "[More info about dataset](https://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.README.1.0.txt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Learn more\n",
    "</font>\n",
    "\n",
    "sklearn.feature_extraction.text.CountVectorizer\n",
    "<br>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "Bag-of-words model\n",
    "<br>\n",
    "https://en.wikipedia.org/wiki/Bag-of-words_model\n",
    "\n",
    "tf–idf\n",
    "<br>\n",
    "https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
    "\n",
    "sklearn.feature_extraction.text.TfidfVectorizer\n",
    "<br>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "Applied Text Mining in Python\n",
    "<br>\n",
    "https://www.coursera.org/learn/python-text-mining/home/welcome\n",
    "\n",
    "Natural Language Processing tutorial\n",
    "<br>\n",
    "https://pythonprogramming.net/tokenizing-words-sentences-nltk-tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color = green >\n",
    "\n",
    "## Next lesson: topic modeling \n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from nltk.corpus import movie_reviews \n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import random\n",
    "import os\n",
    "cwd= os.getcwd()\n",
    "path = os.path.join(cwd,'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "len of texts_pos = 5,331\n",
      "\n",
      " the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \n",
      "\n",
      " the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth . \n",
      "\n",
      " effective but too-tepid biopic\n",
      "\n",
      " if you sometimes like to go to the movies to have fun , wasabi is a good place to start . \n",
      "\n",
      " emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one . \n"
     ]
    }
   ],
   "source": [
    "fn=  os.path.join(path , 'sentimentdata/rt-polarity.neg')\n",
    "with open(fn, \"r\",encoding='utf-8', errors='ignore') as f: # some invalid symbols encountered \n",
    "    content = f.read()  \n",
    "texts_neg=  content.splitlines()\n",
    "\n",
    "fn=  os.path.join(path , 'sentimentdata/rt-polarity.pos')\n",
    "with open(fn, \"r\",encoding='utf-8', errors='ignore') as f: # some invalid symbols encountered \n",
    "    content = f.read()  \n",
    "texts_pos=  content.splitlines()\n",
    "# print('len of texts_neg = {:,}'.format (len(texts_neg)))\n",
    "# for review in texts_neg[:5]:\n",
    "#     print ( '\\n', review)\n",
    "           \n",
    "print('\\n\\n\\nlen of texts_pos = {:,}'.format (len(texts_pos)))\n",
    "for review in texts_pos[:5]:\n",
    "    print ( '\\n', review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "dff = pd.DataFrame(texts_pos)\n",
    "dff[\"y\"]=1\n",
    "df = pd.DataFrame(texts_neg)\n",
    "df[\"y\"]=0\n",
    "df = pd.concat([df, dff])\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[0],df['y'],random_state=0)\n",
    "\n",
    "# vect = CountVectorizer().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier # this is wrapper to incorporate with sklearn using nltk style.\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Convert to nltk classifiers \n",
    "MNNB_classifier= SklearnClassifier(MultinomialNB()) # Note : use ()\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_classifier = SklearnClassifier(LogisticRegression()) \n",
    "\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC # NuSVC - Similar to SVC but uses a parameter to control the number of support vectors.\n",
    "svc_clf = SklearnClassifier(SVC())  \n",
    "lin_svc_clf= SklearnClassifier(LinearSVC())  \n",
    "nu_svc_clf = SklearnClassifier(NuSVC())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "count_vectorizer = CountVectorizer(min_df=5, max_features=50000, ngram_range=(1,3)).fit(X_train) # Note: both limits are included\n",
    "X_train_vectorized = count_vectorizer.transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1:  0.7696814353716587\n",
      "AUC:  0.8323604481784516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1:  0.010211524434719182\n",
      "AUC:  0.6220815556853111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1:  0.760409057706355\n",
      "AUC:  0.8285141405718166\n",
      "f1:  0.7329919531821506\n",
      "AUC:  0.7963450224072334\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression().fit(X_train_vectorized, y_train) \n",
    "predictions = clf.predict(count_vectorizer.transform(X_test)) \n",
    "print('f1: ', f1_score(y_test, predictions)) \n",
    "scores = clf.decision_function(count_vectorizer.transform(X_test)) \n",
    "print('AUC: ', roc_auc_score(y_test, scores))\n",
    "\n",
    "clf = SVC().fit(X_train_vectorized, y_train) \n",
    "predictions = clf.predict(count_vectorizer.transform(X_test)) \n",
    "print('f1: ', f1_score(y_test, predictions)) \n",
    "scores = clf.decision_function(count_vectorizer.transform(X_test)) \n",
    "print('AUC: ', roc_auc_score(y_test, scores))\n",
    "\n",
    "\n",
    "clf = NuSVC().fit(X_train_vectorized, y_train) \n",
    "predictions = clf.predict(count_vectorizer.transform(X_test)) \n",
    "print('f1: ', f1_score(y_test, predictions)) \n",
    "scores = clf.decision_function(count_vectorizer.transform(X_test)) \n",
    "print('AUC: ', roc_auc_score(y_test, scores))\n",
    "\n",
    "clf = LinearSVC().fit(X_train_vectorized, y_train) \n",
    "predictions = clf.predict(count_vectorizer.transform(X_test)) \n",
    "print('f1: ', f1_score(y_test, predictions)) \n",
    "scores = clf.decision_function(count_vectorizer.transform(X_test)) \n",
    "print('AUC: ', roc_auc_score(y_test, scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1:  0.7696814353716587\n",
      "AUC:  0.8323604481784516\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf= LogisticRegression().fit(X_train_vectorized, y_train)\n",
    "predictions = clf.predict(count_vectorizer.transform(X_test)) \n",
    "print('f1: ', f1_score(y_test, predictions)) \n",
    "scores = clf.decision_function(count_vectorizer.transform(X_test)) \n",
    "print('AUC: ', roc_auc_score(y_test, scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
