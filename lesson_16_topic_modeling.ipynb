{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "# Text classification: topic modeling \n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Latent Dirichlet allocation (LDA)\n",
    "\n",
    "</font>\n",
    "\n",
    "Typically used to detect underlying topics in the text documents\n",
    "\n",
    "**Input** : text documents and number of topics \n",
    "<br>\n",
    "**Output**: Distribution of topics for each document (that allows to assign th one with highest probability) and word distribution for each topic \n",
    "\n",
    "**Assumptions**:\n",
    "- Documents with similar topics use similar groups of words \n",
    "- Documents are probability distribution over latent topics \n",
    "- Topics are probability distribution over words\n",
    "\n",
    "\n",
    "<font color = green >\n",
    "\n",
    "#### Generative process\n",
    "<br>\n",
    "</font>\n",
    "\n",
    "LDA considers the every document is created the following way:\n",
    "\n",
    "1) Define number if words in the document\n",
    "<br>\n",
    "2) Chose the topic mixture over the fixed set of topics (e.g. 20% of topic A, 30% of topic A, and 50% of topic A)\n",
    "<br>\n",
    "3) Generate the words by:\n",
    "<br>\n",
    "   -pick the topic based on document's multinomial distribution \n",
    "<br>\n",
    "   -pick the word based on topic's multinomial distribution \n",
    "\n",
    "<img src = \"img/topics_modeling.png\" height=500 width= 800 align=\"left\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color = green >\n",
    "\n",
    "#### Recall\n",
    "</font>\n",
    "\n",
    "\n",
    "#### Binomial distribution\n",
    "\n",
    "$$p(k/n)\\quad =\\quad C^{ k }_{ n }\\cdot p^{ k }(1-p)^{ n-k }\\quad =\\quad \\frac { n! }{ k!(n-k)! } p^{ k }(1-p)^{ n-k }$$\n",
    "\n",
    "Example: Probability of 6 of 10 for fear coin: \n",
    "$$p(6,4)\\quad =\\quad C^{ 6 }_{ 10 }\\cdot {0.5}^{ 6 }(0.5)^{ 4 }\\quad = 210 \\cdot 0.015625 \\cdot 0.0625 = 0.205078125$$\n",
    "\n",
    "\n",
    "#### Multinomial distribution\n",
    "\n",
    "$$p(n_{ 1 }n_{ 2 }...n_{ k })\\quad =\\quad \\frac { n! }{ n_{ 1 }!n_{ 2 }!...n_{ k }! } p^{ n_{ 1 } }_{ 1 }p^{ n_{ 2 } }_{ 2 }...p^{ n_{ k } }_{ k }$$\n",
    "\n",
    "Example (three outcomes): <br>\n",
    "n = 12 (12 games are played),<br>\n",
    "n1 = 7 (number won by Player A),<br>\n",
    "n2 = 2 (number won by Player B),<br>\n",
    "n3 = 3 (the number drawn),<br>\n",
    "p1 = 0.4 (probability Player A wins)<br>\n",
    "p2 = 0.35(probability Player B wins)<br>\n",
    "p3 = 0.25(probability of a draw)<br>\n",
    "$$p(7,2,3)\\quad =\\quad \\frac {12!}{ 7! \\cdot 2! \\cdot3 ! }  \\cdot 0.4^{7} \\cdot 0.35^{2} \\cdot0.25^{3} = 0.0248$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Maximul Likelihood Estimation\n",
    "\n",
    "</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "**Recall** \n",
    "<br> Known are text documents and number $K$ of topics \n",
    "\n",
    "**Target**:\n",
    "<br>Within all possible topics distribution for all documemnts and all possible words distribution for topics, shoose the one wich maximizes probability of all text documents.\n",
    "\n",
    "**Approach** :\n",
    "<br>\n",
    "1) Randomly assign each word of each document to $K$ topics \n",
    "<br>\n",
    "2) Iterate the following process till convergence (steady assignments of w to topics) \n",
    "<br>$\\quad\\quad$For each document $d$: \n",
    "<br>\n",
    "    $\\quad\\quad\\bullet$ Assume that all topic assignment except current one are correct     \n",
    "    $\\quad\\quad\\bullet$ For each word $w$ in $d$:           \n",
    "    $\\quad\\quad\\quad$ - For every topic $t$ compare the the score for hypothesis that w is in this topic $t$:\n",
    "   <br>$\\quad\\quad\\quad\\quad\\quad score (t) =  p(t | d) \\cdot p (w |t),$\n",
    "   <br>$\\quad\\quad\\quad\\quad p(t|d)$ is proportion of all words in d from t,\n",
    "    <br>$\\quad\\quad\\quad\\quad p(w|t)$ is share of word w in topic t.  \n",
    "    $\\quad\\quad\\quad$ - Assign the word w to the topic with max score\n",
    "    <br>$\\quad\\quad\\bullet$ Iterate through all $w$ in $d$:           \n",
    "$\\quad\\quad$Iterate through all $d$\n",
    "\n",
    "Te results is matrix of distribution of words in topics  \n",
    "Note: The computed topics are just words distribution, i.e. need to summarize them somehow. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Gensim LDA \n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Define the text documents \n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\"\n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Tokenize, clean, and stem\n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brocolli',\n",
       " 'good',\n",
       " 'eat',\n",
       " '.',\n",
       " 'brother',\n",
       " 'like',\n",
       " 'eat',\n",
       " 'good',\n",
       " 'brocolli',\n",
       " ',',\n",
       " 'mother',\n",
       " '.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_stop  = set(stopwords.words('english'))\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "def tokenize(doc_set):\n",
    "    texts = []\n",
    "    for doc in doc_set:\n",
    "        # tokenize document string\n",
    "        raw = doc.lower()\n",
    "        tokens = word_tokenize(raw)\n",
    "\n",
    "        # remove stop words from tokens\n",
    "        tokens = [token for token in tokens if token not in en_stop]\n",
    "\n",
    "        # stem tokens\n",
    "        tokens = [p_stemmer.stem(token) for token in tokens]\n",
    "\n",
    "        # add tokens to list\n",
    "        texts.append(tokens)\n",
    "    return texts\n",
    "\n",
    "texts = tokenize(doc_set)\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Convert tokenized documents into a \"id <-> term\" dictionary\n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.corpora.dictionary.Dictionary'> Dictionary(34 unique tokens: [',', '.', 'brocolli', 'brother', 'eat']...)\n",
      "0 ,\n",
      "1 .\n",
      "2 brocolli\n",
      "3 brother\n",
      "4 eat\n",
      "5 good\n",
      "6 like\n",
      "7 mother\n",
      "8 around\n",
      "9 basebal\n",
      "10 drive\n",
      "11 lot\n",
      "12 practic\n",
      "13 spend\n",
      "14 time\n",
      "15 blood\n",
      "16 caus\n",
      "17 expert\n",
      "18 health\n",
      "19 increas\n",
      "20 may\n",
      "21 pressur\n",
      "22 suggest\n",
      "23 tension\n",
      "24 better\n",
      "25 feel\n",
      "26 never\n",
      "27 often\n",
      "28 perform\n",
      "29 school\n",
      "30 seem\n",
      "31 well\n",
      "32 profession\n",
      "33 say\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts) # this is alternative way - without using count vectorizer\n",
    "print (type(dictionary), dictionary)\n",
    "for k,w in dictionary.items():\n",
    "    print (k,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Create gensim corpus\n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "convert tokenized documents into a document-term matrix\n",
      "[(0, 1), (1, 2), (2, 2), (3, 1), (4, 2), (5, 2), (6, 1), (7, 1)]\n",
      "[(1, 1), (3, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1)]\n",
      "[(1, 1), (10, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1)]\n",
      "[(0, 1), (1, 1), (3, 1), (7, 1), (10, 1), (21, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1)]\n",
      "[(1, 1), (2, 1), (5, 1), (18, 2), (32, 1), (33, 1)]\n"
     ]
    }
   ],
   "source": [
    "print ('\\nconvert tokenized documents into a document-term matrix')\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts] # id and count\n",
    "for item in corpus:\n",
    "    print (item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Generate LDA model\n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(\n",
    "    corpus, num_topics=2, id2word=dictionary, passes=20, random_state= 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Review topics \n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.098*\".\" + 0.076*\"brocolli\" + 0.076*\"good\" + 0.055*\"mother\" + 0.055*\"brother\" + 0.054*\"health\" + 0.054*\"eat\" + 0.033*\",\" + 0.033*\"like\" + 0.033*\"spend\"'),\n",
       " (1,\n",
       "  '0.060*\"drive\" + 0.059*\"pressur\" + 0.059*\".\" + 0.036*\",\" + 0.036*\"never\" + 0.036*\"often\" + 0.036*\"increas\" + 0.036*\"perform\" + 0.036*\"seem\" + 0.036*\"well\"')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topics(num_topics=2,num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Classify the new text \n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "get topics:\n",
      "[(0, 0.37417665), (1, 0.6258233)]\n"
     ]
    }
   ],
   "source": [
    "test_doc_list = [\"Some experts suggest that car may cause increased blood pressure. professionals say that brocolli is good \"]\n",
    "test_texts = tokenize(test_doc_list)\n",
    "test_corpus = [dictionary.doc2bow(text) for text in test_texts ]\n",
    "test_doc_topics = ldamodel.get_document_topics(test_corpus)\n",
    "print ('\\nget topics:')\n",
    "for el in test_doc_topics: # loop over all tests in provided list\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Sample of topic modeling on large dataset\n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Load \"News\" data \n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd= os.getcwd()\n",
    "path = os.path.join(cwd,'')\n",
    "fn=  os.path.join(path , 'newsgroups')\n",
    "\n",
    "with open(fn, 'rb') as f:\n",
    "    newsgroup_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Review data\n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "len of documents = 2,000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The best group to keep you informed is the Crohn's and Colitis Foundation\\nof America.  I do not know if the UK has a similar organization.  The\\naddress of\\nthe CCFA is \\n\\nCCFA\\n444 Park Avenue South\\n11th Floor\\nNew York, NY  10016-7374\\nUSA\\n\\nThey have a lot of information available and have a number of newsletters.\\n \\nGood Luck.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (type(newsgroup_data))\n",
    "print ('len of documents = {:,}\\n'.format(len(newsgroup_data)))\n",
    "\n",
    "newsgroup_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Define custom vectorizer\n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_words_pattern = r\"\\b\\w{3,}\\b\"\n",
    "vectorizer = CountVectorizer(\n",
    "    min_df=20, \n",
    "    stop_words='english',\n",
    "    token_pattern=three_words_pattern) \n",
    "vectorizer.fit(newsgroup_data)\n",
    "newsgroup_data_vectorized= vectorizer.transform(newsgroup_data)\n",
    "corpus = gensim.matutils.Sparse2Corpus(newsgroup_data_vectorized, documents_columns=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Review feratures \n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of features = 902\n",
      "\n",
      "['000', '100', '1990', '1992', '1993', '200', '2nd', '300', '400', '486', '500', '800', 'ability', 'able', 'accept', 'accepted', 'access', 'according', 'actual', 'actually', 'add', 'addition', 'additional', 'address', 'advance', 'advice', 'age', 'ago', 'agree', 'ahead', 'air', 'allow', 'alt', 'america', 'american', 'answer', 'answers', 'anybody', 'apparently', 'appears']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print ('len of features = {:,}\\n'.format(len(vectorizer.get_feature_names())))\n",
    "print (vectorizer.get_feature_names()[:40])\n",
    "type(vectorizer.get_feature_names()[:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Vectorize data set\n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 23)\t1\n",
      "  (0, 33)\t1\n",
      "  (0, 58)\t1\n",
      "  (0, 76)\t1\n",
      "  (0, 326)\t1\n",
      "  (0, 335)\t1\n",
      "  (0, 386)\t1\n",
      "  (0, 409)\t1\n",
      "  (0, 451)\t1\n",
      "  (0, 456)\t1\n",
      "  (0, 515)\t1\n",
      "  (0, 529)\t1\n",
      "  (0, 545)\t1\n",
      "  (0, 727)\t1\n",
      "  (0, 843)\t1\n",
      "  (0, 900)\t1\n",
      "  (1, 33)\t1\n",
      "  (1, 34)\t1\n",
      "  (1, 84)\t1\n",
      "  (1, 184)\t1\n",
      "  (1, 201)\t1\n",
      "  (1, 214)\t1\n",
      "  (1, 231)\t2\n",
      "  (1, 241)\t1\n",
      "  (1, 324)\t1\n",
      "  :\t:\n",
      "  (1998, 622)\t1\n",
      "  (1998, 625)\t3\n",
      "  (1998, 688)\t1\n",
      "  (1998, 698)\t2\n",
      "  (1998, 726)\t1\n",
      "  (1998, 804)\t1\n",
      "  (1998, 805)\t1\n",
      "  (1998, 810)\t10\n",
      "  (1998, 813)\t2\n",
      "  (1998, 814)\t1\n",
      "  (1998, 816)\t1\n",
      "  (1998, 818)\t1\n",
      "  (1998, 844)\t1\n",
      "  (1998, 882)\t2\n",
      "  (1998, 899)\t1\n",
      "  (1999, 171)\t1\n",
      "  (1999, 194)\t1\n",
      "  (1999, 205)\t1\n",
      "  (1999, 213)\t1\n",
      "  (1999, 276)\t2\n",
      "  (1999, 308)\t1\n",
      "  (1999, 344)\t1\n",
      "  (1999, 669)\t1\n",
      "  (1999, 832)\t1\n",
      "  (1999, 874)\t1\n"
     ]
    }
   ],
   "source": [
    "newsgroup_data_vectorized= vectorizer.transform(newsgroup_data)\n",
    "corpus = gensim.matutils.Sparse2Corpus(newsgroup_data_vectorized, documents_columns=False)\n",
    "print (newsgroup_data_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Create gensim corpus\n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(23, 1),\n",
       "  (33, 1),\n",
       "  (58, 1),\n",
       "  (76, 1),\n",
       "  (326, 1),\n",
       "  (335, 1),\n",
       "  (386, 1),\n",
       "  (409, 1),\n",
       "  (451, 1),\n",
       "  (456, 1),\n",
       "  (515, 1),\n",
       "  (529, 1),\n",
       "  (545, 1),\n",
       "  (727, 1),\n",
       "  (843, 1),\n",
       "  (900, 1)],\n",
       " [(33, 1),\n",
       "  (34, 1),\n",
       "  (84, 1),\n",
       "  (184, 1),\n",
       "  (201, 1),\n",
       "  (214, 1),\n",
       "  (231, 2),\n",
       "  (241, 1),\n",
       "  (324, 1),\n",
       "  (332, 1),\n",
       "  (359, 1),\n",
       "  (363, 1),\n",
       "  (365, 1),\n",
       "  (409, 1),\n",
       "  (430, 3),\n",
       "  (451, 1),\n",
       "  (475, 1),\n",
       "  (492, 2),\n",
       "  (525, 2),\n",
       "  (605, 1),\n",
       "  (633, 2),\n",
       "  (642, 1),\n",
       "  (674, 1),\n",
       "  (688, 1),\n",
       "  (709, 1),\n",
       "  (750, 1),\n",
       "  (777, 1),\n",
       "  (823, 1),\n",
       "  (838, 1),\n",
       "  (874, 1),\n",
       "  (896, 1)],\n",
       " [(25, 1),\n",
       "  (26, 1),\n",
       "  (63, 1),\n",
       "  (120, 1),\n",
       "  (231, 1),\n",
       "  (297, 1),\n",
       "  (326, 1),\n",
       "  (344, 1),\n",
       "  (373, 1),\n",
       "  (423, 1),\n",
       "  (442, 1),\n",
       "  (444, 1),\n",
       "  (448, 2),\n",
       "  (465, 1),\n",
       "  (572, 1),\n",
       "  (653, 1),\n",
       "  (659, 1),\n",
       "  (714, 1),\n",
       "  (777, 1),\n",
       "  (779, 1),\n",
       "  (781, 1),\n",
       "  (818, 1),\n",
       "  (836, 1),\n",
       "  (855, 1),\n",
       "  (890, 1),\n",
       "  (898, 1)],\n",
       " [(4, 1),\n",
       "  (17, 2),\n",
       "  (18, 1),\n",
       "  (22, 1),\n",
       "  (42, 1),\n",
       "  (48, 2),\n",
       "  (68, 1),\n",
       "  (78, 1),\n",
       "  (86, 1),\n",
       "  (94, 1),\n",
       "  (117, 1),\n",
       "  (119, 1),\n",
       "  (122, 1),\n",
       "  (148, 1),\n",
       "  (155, 1),\n",
       "  (169, 1),\n",
       "  (210, 2),\n",
       "  (232, 1),\n",
       "  (242, 7),\n",
       "  (262, 1),\n",
       "  (297, 1),\n",
       "  (348, 2),\n",
       "  (360, 3),\n",
       "  (367, 2),\n",
       "  (374, 1),\n",
       "  (378, 2),\n",
       "  (384, 1),\n",
       "  (386, 4),\n",
       "  (392, 1),\n",
       "  (397, 2),\n",
       "  (403, 1),\n",
       "  (423, 1),\n",
       "  (426, 1),\n",
       "  (438, 1),\n",
       "  (440, 3),\n",
       "  (462, 1),\n",
       "  (466, 1),\n",
       "  (470, 2),\n",
       "  (471, 1),\n",
       "  (483, 1),\n",
       "  (486, 2),\n",
       "  (506, 1),\n",
       "  (509, 1),\n",
       "  (510, 2),\n",
       "  (515, 3),\n",
       "  (553, 2),\n",
       "  (558, 2),\n",
       "  (572, 1),\n",
       "  (588, 2),\n",
       "  (612, 5),\n",
       "  (615, 1),\n",
       "  (616, 1),\n",
       "  (635, 1),\n",
       "  (641, 1),\n",
       "  (646, 1),\n",
       "  (652, 1),\n",
       "  (655, 1),\n",
       "  (688, 2),\n",
       "  (695, 3),\n",
       "  (715, 3),\n",
       "  (727, 1),\n",
       "  (746, 1),\n",
       "  (762, 1),\n",
       "  (765, 1),\n",
       "  (768, 1),\n",
       "  (780, 1),\n",
       "  (793, 1),\n",
       "  (798, 2),\n",
       "  (799, 3),\n",
       "  (818, 1),\n",
       "  (844, 4),\n",
       "  (894, 1),\n",
       "  (901, 2)],\n",
       " [(334, 3), (466, 1)]]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = gensim.matutils.Sparse2Corpus(newsgroup_data_vectorized, documents_columns=False)\n",
    "# comparing to using corpora.Dictionary:\n",
    "# corpus = [dictionary.doc2bow(text) for text in texts] \n",
    "[item for item in corpus][:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Create id2word dictionary\n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_map = dict((v, k) for k, v in vectorizer.vocabulary_.items()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Generate LDA model\n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel (corpus, num_topics=6, id2word=id_map, passes=25, random_state=34)\n",
    "# Comparing to corpora.Dictionary\n",
    "# ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word=dictionary, passes=20, random_state= 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Review topics\n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.025*\"edu\" + 0.019*\"com\" + 0.018*\"use\" + 0.018*\"thanks\" + 0.016*\"does\" + 0.015*\"know\" + 0.011*\"mail\" + 0.010*\"apple\" + 0.009*\"help\" + 0.008*\"want\"'),\n",
       " (1,\n",
       "  '0.061*\"drive\" + 0.039*\"disk\" + 0.030*\"scsi\" + 0.027*\"drives\" + 0.027*\"hard\" + 0.025*\"controller\" + 0.021*\"card\" + 0.018*\"rom\" + 0.016*\"cable\" + 0.016*\"floppy\"'),\n",
       " (2,\n",
       "  '0.024*\"people\" + 0.022*\"god\" + 0.013*\"atheism\" + 0.012*\"think\" + 0.012*\"believe\" + 0.012*\"don\" + 0.010*\"does\" + 0.010*\"just\" + 0.009*\"argument\" + 0.009*\"say\"'),\n",
       " (3,\n",
       "  '0.023*\"game\" + 0.021*\"year\" + 0.020*\"team\" + 0.013*\"games\" + 0.013*\"play\" + 0.011*\"good\" + 0.011*\"don\" + 0.010*\"think\" + 0.010*\"season\" + 0.010*\"players\"'),\n",
       " (4,\n",
       "  '0.035*\"space\" + 0.019*\"nasa\" + 0.018*\"data\" + 0.013*\"information\" + 0.013*\"available\" + 0.013*\"center\" + 0.011*\"ground\" + 0.010*\"research\" + 0.010*\"000\" + 0.010*\"new\"'),\n",
       " (5,\n",
       "  '0.017*\"just\" + 0.017*\"like\" + 0.016*\"don\" + 0.012*\"car\" + 0.012*\"time\" + 0.011*\"think\" + 0.011*\"good\" + 0.010*\"know\" + 0.008*\"way\" + 0.008*\"people\"')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel (corpus, num_topics=6, id2word=id_map, passes=25, random_state=34)\n",
    "ldamodel.print_topics(num_topics=6,num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Name topics\n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_names= ['Education', 'Computers & IT', 'Religion', 'Sports', 'Science','Society & Lifestyle']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Classify the new text \n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc = [\"\\n\\nIt's my understanding that the freezing will start to occur because \\\n",
    "of the\\ngrowing distance of Pluto and Charon from the Sun, due to it's\\nelliptical orbit. \\\n",
    "It is not due to shadowing effects. \\n\\n\\nPluto can shadow Charon, and vice-versa.\\n\\nGeorge \\\n",
    "Krumins\\n-- \"] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0.033414923),\n",
       "  (1, 0.03333784),\n",
       "  (2, 0.033519648),\n",
       "  (3, 0.033781353),\n",
       "  (4, 0.83230925),\n",
       "  (5, 0.033636983)]]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vectorized= vectorizer.transform(new_doc) # input param is list\n",
    "new_doc_corpus = gensim.matutils.Sparse2Corpus(doc_vectorized, documents_columns=False)\n",
    "doc_topics = ldamodel.get_document_topics(new_doc_corpus)\n",
    "list(doc_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Science'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def elicit_topic_name(doc_topics):    \n",
    "    return topics_names[np.squeeze(np.array(doc_topics))[:,1].argmax()]\n",
    "elicit_topic_name(doc_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Home Task \n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Topic Modeling \n",
    "\n",
    "</font>\n",
    "\n",
    "[voted-kaggle-dataset](https://www.kaggle.com/canggih/voted-kaggle-dataset/version/2#voted-kaggle-dataset.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of texts= 2,150\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "cwd= os.getcwd()\n",
    "path = os.path.join(cwd,'')\n",
    "fn=  os.path.join(path , 'voted-kaggle-dataset.csv')\n",
    "df = pd.read_csv(fn)\n",
    "\n",
    "\n",
    "# with open('test.csv', 'rb') as f:\n",
    "#     data = list(csv.reader(f))\n",
    "# data=df['Description']\n",
    "# for row in data:\n",
    "#     writer = csv.writer(open(\"test1.csv\", \"wb\"))\n",
    "#     writer.writerows(row)\n",
    "print ('len of texts= {:,}'.format(len(df)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=df['Description'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datasets',\n",
       " 'contains',\n",
       " 'transactions',\n",
       " 'made',\n",
       " 'cre',\n",
       " 'ultimate',\n",
       " 'soccer',\n",
       " 'database',\n",
       " 'data',\n",
       " 'analysis',\n",
       " 'name',\n",
       " 'description',\n",
       " 'dtype',\n",
       " 'object']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_stop  = set(stopwords.words('english'))\n",
    "p_stemmer = PorterStemmer()\n",
    "pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*')\n",
    "import re\n",
    "\n",
    "# def fix_Plan(df):\n",
    "#     for text in df:\n",
    "#         tokens = re.sub(\"[^a-zA-Z]\",  # Search for all non-letters\n",
    "#                                   \" \",          # Replace all non-letters with spaces\n",
    "#                                   str(text))\n",
    "\n",
    "#         words = tokens.lower().split()     \n",
    "#         stops = set(stopwords.words(\"english\"))      \n",
    "#         meaningful_words = [w for w in words if not w in stops]  \n",
    "#     return meaningful_words\n",
    "# tokens=fix_Plan(df)\n",
    "\n",
    "\n",
    "def tokenize(df):\n",
    "    texts = []\n",
    "    for doc in df:\n",
    "        # tokenize document string\n",
    "        tokens = re.sub(\"[^a-zA-Z]\",  # Search for all non-letters\n",
    "                                  \" \",          # Replace all non-letters with spaces\n",
    "                                  str(text))\n",
    "\n",
    "        words = tokens.lower().split()     \n",
    "        stops = set(stopwords.words(\"english\"))      \n",
    "        tokens = [w for w in words if not w in stops]\n",
    "        texts.append(tokens)\n",
    "    return texts\n",
    "\n",
    "texts = tokenize(df)\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = CountVectorizer(\n",
    "#     min_df=20) \n",
    "# vectorizer.fit(texts)\n",
    "three_words_pattern = r\"\\b\\w{3,}\\b\" \n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts] # id and count\n",
    "# corpus = gensim.matutils.Sparse2Corpus(newsgroup_data_vectorized, documents_columns=False)\n",
    "# newsgroup_data_vectorized= v.transform(x)\n",
    "# id_map = dict((v, k) for k, v in v.vocabulary_.items()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.082*\"description\" + 0.079*\"transactions\" + 0.079*\"ultimate\" + 0.079*\"name\" + 0.075*\"data\" + 0.074*\"analysis\" + 0.074*\"soccer\" + 0.073*\"database\" + 0.070*\"datasets\" + 0.068*\"dtype\"'),\n",
       " (1,\n",
       "  '0.085*\"cre\" + 0.083*\"contains\" + 0.081*\"object\" + 0.077*\"made\" + 0.075*\"dtype\" + 0.073*\"datasets\" + 0.070*\"database\" + 0.069*\"soccer\" + 0.069*\"analysis\" + 0.067*\"data\"')]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel (corpus, num_topics=2, id2word=dictionary, passes=25, random_state=34)\n",
    "# ldamodel = gensim.models.ldamodel.LdaModel(\n",
    "#     corpus, num_topics=2, id2word=dictionary, passes=20, random_state= 0)\n",
    "ldamodel.print_topics(num_topics=2,num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x36412 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 164 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Learn more\n",
    "</font>\n",
    "\n",
    "Latent Dirichlet allocation\n",
    "<br>\n",
    "https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color = green >\n",
    "\n",
    "## Next lesson: Clustering \n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
